{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MundoDodgeballAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHZbBSaEV2FeGPM6l4F9RR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alpharaoh/Machine-Learning/blob/main/MundoDodgeballAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnNOtpVp9lQG"
      },
      "source": [
        "# Mundo Dodgeball AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp5303g89tir"
      },
      "source": [
        "Get YOLOv5 dependances..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcnw0ceoMQ_0"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdkDno6NOR0P"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADKL_Gp_el_l"
      },
      "source": [
        "-------------------------------------------------------------------------------\n",
        "\n",
        "Download Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiZRYFVqelq4",
        "outputId": "ba43b298-11bf-4efe-c7dd-bbdda549a862"
      },
      "source": [
        "!pip install PyDrive\n",
        "\n",
        "\n",
        "# verify\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (5.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (51.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdUm0maWf7pS"
      },
      "source": [
        "download = drive.CreateFile({\"id\": \"1P7nh4llt1DPfPXNMcyXvLnE_ydXWSIw_\"})\n",
        "download.GetContentFile(\"yolo_dataset_v2.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_u-gYV7evjl"
      },
      "source": [
        "-------------------------------------------------------------------------------\n",
        "\n",
        "Check we are using a Google Colab supplied GPU (common to be a Tesla T4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yr1K4UcSUNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ace060d-9de3-4a71-f557-c4da160ba2ad"
      },
      "source": [
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.7.0+cu101 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15079MB, multi_processor_count=40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP9BUBdzjXO6"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Download Apex for faster learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvylU10XgDSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d244b6-21f9-4b6e-a508-b1304353ed10"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 7872, done.\u001b[K\n",
            "remote: Total 7872 (delta 0), reused 0 (delta 0), pack-reused 7872\u001b[K\n",
            "Receiving objects: 100% (7872/7872), 13.98 MiB | 29.21 MiB/s, done.\n",
            "Resolving deltas: 100% (5374/5374), done.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-5gvimkug\n",
            "Created temporary directory: /tmp/pip-req-tracker-c55wne47\n",
            "Created requirements tracker '/tmp/pip-req-tracker-c55wne47'\n",
            "Created temporary directory: /tmp/pip-install-at4fja6x\n",
            "Processing /content/yolov5/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-bu4hxy4a\n",
            "  Added file:///content/yolov5/apex to build tracker '/tmp/pip-req-tracker-c55wne47'\n",
            "    Running setup.py (path:/tmp/pip-req-build-bu4hxy4a/setup.py) egg_info for package from file:///content/yolov5/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-bu4hxy4a/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-bu4hxy4a/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-bu4hxy4a has version 0.1, which satisfies requirement apex==0.1 from file:///content/yolov5/apex\n",
            "  Removed apex==0.1 from file:///content/yolov5/apex from build tracker '/tmp/pip-req-tracker-c55wne47'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-a37n07ft\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-bu4hxy4a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-bu4hxy4a/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-a37n07ft/install-record.txt --single-version-externally-managed --compile --user --prefix=\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-bu4hxy4a/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /root/.local/lib/python3.6/site-packages/apex\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /root/.local/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-a37n07ft/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-bu4hxy4a\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-c55wne47'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3uj-bt6jdn2"
      },
      "source": [
        "Train model...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYvpcPx82oM-",
        "outputId": "916fef2a-20d5-4e76-a5b4-c1b50317a506"
      },
      "source": [
        "!python train.py --img 416 --batch 64 --epochs 100 \\\n",
        "  --data ./data/mundoAI.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt \\\n",
        "  --name yolov5s_mundo --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using torch 1.7.0+cu101 CUDA:0 (Tesla T4, 15079.75MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=64, bucket='', cache_images=True, cfg='./models/yolov5s.yaml', data='./data/mundoAI.yaml', device='', epochs=100, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[416, 416], local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='yolov5s_mundo', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/yolov5s_mundo2', single_cls=False, sync_bn=False, total_batch_size=64, weights='yolov5s.pt', workers=8, world_size=1)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "2020-12-28 13:48:55.503501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 283 layers, 7257791 parameters, 7257791 gradients, 16.8 GFLOPS\n",
            "\n",
            "Transferred 362/370 items from yolov5s.pt\n",
            "Optimizer groups: 62 .bias, 70 conv.weight, 59 other\n",
            "Scanning '../yolo_dataset_v2/labels/train.cache' for images and labels... 14510 found, 0 missing, 0 empty, 0 corrupted: 100% 14510/14510 [00:00<00:00, 128666704.10it/s]\n",
            "Caching images (4.2GB): 100% 14510/14510 [04:37<00:00, 52.24it/s]\n",
            "Scanning '../yolo_dataset_v2/labels/val.cache' for images and labels... 1556 found, 0 missing, 0 empty, 0 corrupted: 100% 1556/1556 [00:00<00:00, 9858515.14it/s]\n",
            "Caching images (0.5GB): 100% 1556/1556 [00:41<00:00, 37.27it/s]\n",
            "Plotting labels... \n",
            "NumExpr defaulting to 2 threads.\n",
            "\n",
            "Analyzing anchors... anchors/target = 4.96, Best Possible Recall (BPR) = 0.9899\n",
            "Image sizes 416 train, 416 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/yolov5s_mundo2\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      0/99     4.03G   0.08434   0.04689   0.01123    0.1425       224       416: 100% 227/227 [01:47<00:00,  2.12it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:10<00:00,  2.43it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.267       0.865       0.697       0.295\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      1/99      5.3G   0.05536   0.03724  0.003745   0.09635       246       416: 100% 227/227 [01:42<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.74it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.574        0.92       0.896       0.461\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      2/99      5.3G   0.04587   0.03441  0.002883   0.08317       220       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.70it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.644       0.941       0.932       0.565\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      3/99      5.3G   0.03898   0.03247  0.002469   0.07392       249       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.719       0.944       0.942       0.604\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      4/99      5.3G    0.0349   0.03099  0.002224   0.06811       220       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.77it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.739       0.945       0.947       0.631\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      5/99      5.3G   0.03167   0.02924  0.001962   0.06287       256       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.787       0.949       0.953        0.67\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      6/99      5.3G   0.02984   0.02819  0.001815   0.05985       249       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.73it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.764       0.957       0.959       0.686\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      7/99      5.3G   0.02879    0.0273  0.001729   0.05782       242       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.79it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.806       0.955       0.959       0.698\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      8/99      5.3G   0.02806   0.02684  0.001703   0.05661       320       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.797       0.955        0.96       0.708\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      9/99      5.3G   0.02727   0.02628  0.001658    0.0552       230       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.791       0.963       0.965       0.723\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     10/99      5.3G    0.0264   0.02563  0.001571   0.05359       235       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.794       0.963       0.964       0.726\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     11/99      5.3G   0.02626   0.02565  0.001542   0.05345       297       416: 100% 227/227 [01:45<00:00,  2.15it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.76it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.821       0.962       0.968       0.729\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     12/99      5.3G   0.02579   0.02515   0.00152   0.05246       242       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.77it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.798       0.966       0.969       0.745\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     13/99      5.3G    0.0253   0.02485  0.001501   0.05165       236       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.80it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.812       0.966        0.97       0.743\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     14/99      5.3G   0.02498   0.02462  0.001464   0.05106       287       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.805        0.97       0.973       0.748\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     15/99      5.3G   0.02483    0.0243  0.001445   0.05057       212       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.815       0.967       0.971       0.755\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     16/99      5.3G   0.02469   0.02412  0.001456   0.05027       261       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.76it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.82        0.97       0.973       0.758\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     17/99      5.3G   0.02416   0.02403  0.001421   0.04961       244       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.828       0.968       0.972       0.759\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     18/99      5.3G   0.02401   0.02387  0.001432   0.04931       225       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.88it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.817       0.968       0.972       0.767\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     19/99      5.3G    0.0238   0.02369  0.001391   0.04888       262       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.829       0.969       0.972       0.768\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     20/99      5.3G   0.02374   0.02351  0.001379   0.04863       249       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.88it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.812       0.973       0.974       0.772\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     21/99      5.3G   0.02349   0.02356  0.001369   0.04841       234       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.829        0.97       0.974       0.778\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     22/99      5.3G   0.02321   0.02307   0.00136   0.04765       227       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.836       0.968       0.973       0.778\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     23/99      5.3G   0.02296   0.02294  0.001355   0.04725       244       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.834       0.972       0.976       0.786\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     24/99      5.3G   0.02297   0.02301  0.001377   0.04736       243       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.829       0.973       0.975       0.787\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     25/99      5.3G   0.02278   0.02273  0.001338   0.04685       274       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.828       0.976       0.979       0.787\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     26/99      5.3G   0.02256   0.02269   0.00132   0.04657       231       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.73it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.835       0.974       0.977       0.789\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     27/99      5.3G   0.02246   0.02254  0.001309    0.0463       263       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.89it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.829       0.974       0.978       0.796\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     28/99      5.3G   0.02245   0.02233  0.001301   0.04609       243       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.832       0.975       0.978         0.8\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     29/99      5.3G   0.02232   0.02234  0.001312   0.04597       183       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.833       0.973       0.979       0.798\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     30/99      5.3G   0.02197   0.02219  0.001287   0.04544       246       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.836       0.974       0.978       0.805\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     31/99      5.3G   0.02189   0.02211  0.001258   0.04526       254       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.833       0.976       0.979       0.806\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     32/99      5.3G   0.02189   0.02189  0.001261   0.04504       241       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.839       0.976        0.98       0.808\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     33/99      5.3G   0.02173     0.022  0.001254   0.04498       216       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.833       0.976       0.979       0.812\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     34/99      5.3G   0.02176   0.02173  0.001229   0.04471       236       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.834       0.978       0.981       0.815\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     35/99      5.3G   0.02152   0.02176   0.00125   0.04453       276       416: 100% 227/227 [01:43<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.78it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.831       0.978       0.979       0.814\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     36/99      5.3G   0.02141   0.02165   0.00125    0.0443       243       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.91it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.841       0.979       0.982       0.818\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     37/99      5.3G    0.0212   0.02153  0.001227   0.04396       241       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.84       0.977        0.98       0.818\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     38/99      5.3G   0.02124   0.02138  0.001234   0.04386       264       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.842       0.978       0.982       0.819\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     39/99      5.3G   0.02108   0.02138   0.00121   0.04367       297       416: 100% 227/227 [01:43<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.84       0.979       0.983       0.822\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     40/99      5.3G   0.02109   0.02131  0.001214   0.04361       247       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.88it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.84        0.98       0.983       0.823\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     41/99      5.3G   0.02077   0.02113  0.001216   0.04312       238       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.91it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.844        0.98       0.983       0.827\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     42/99      5.3G    0.0206   0.02093  0.001192   0.04272       256       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.88it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.845        0.98       0.983        0.83\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     43/99      5.3G   0.02086   0.02115  0.001225   0.04323       258       416: 100% 227/227 [01:43<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.843        0.98       0.983       0.829\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     44/99      5.3G   0.02059   0.02077  0.001201   0.04256       250       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.84        0.98       0.983       0.832\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     45/99      5.3G   0.02055   0.02081  0.001172   0.04254       226       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.841        0.98       0.984       0.834\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     46/99      5.3G   0.02038   0.02067  0.001163   0.04221       229       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.90it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.843        0.98       0.984       0.834\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     47/99      5.3G   0.02021   0.02067   0.00118   0.04206       258       416: 100% 227/227 [01:45<00:00,  2.16it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.846       0.981       0.984       0.836\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     48/99      5.3G   0.02021   0.02062  0.001171   0.04201       225       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.984       0.837\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     49/99      5.3G   0.02002   0.02022   0.00115   0.04139       253       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.84it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.846        0.98       0.984       0.837\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     50/99      5.3G   0.01997   0.02038  0.001185   0.04154       247       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.848        0.98       0.984       0.839\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     51/99      5.3G   0.02002   0.02042  0.001169   0.04161       257       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.846        0.98       0.984        0.84\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     52/99      5.3G   0.01983   0.02019  0.001182    0.0412       264       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.84it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.846        0.98       0.984        0.84\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     53/99      5.3G    0.0196   0.02017  0.001123   0.04089       290       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.847        0.98       0.984       0.842\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     54/99      5.3G   0.01955   0.02015  0.001149   0.04085       238       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.846        0.98       0.984       0.843\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     55/99      5.3G   0.01954   0.01999  0.001165    0.0407       216       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.848       0.981       0.984       0.845\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     56/99      5.3G   0.01939   0.01994  0.001153   0.04048       286       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.91it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.847       0.982       0.984       0.844\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     57/99      5.3G   0.01935   0.01986  0.001121   0.04033       276       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.848       0.982       0.985       0.845\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     58/99      5.3G    0.0193   0.01976  0.001137    0.0402       245       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.849        0.98       0.984       0.845\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     59/99      5.3G   0.01914   0.01964  0.001121   0.03989       250       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85       0.981       0.984       0.845\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     60/99      5.3G    0.0191   0.01953  0.001117   0.03974       249       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851        0.98       0.984       0.847\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     61/99      5.3G   0.01909   0.01951  0.001099    0.0397       234       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.849        0.98       0.985       0.848\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     62/99      5.3G   0.01888   0.01948  0.001102   0.03946       257       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.79it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.847       0.979       0.984       0.848\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     63/99      5.3G   0.01878   0.01932  0.001103    0.0392       228       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.80it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.984       0.849\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     64/99      5.3G   0.01874   0.01938  0.001099   0.03922       251       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.849       0.979       0.984        0.85\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     65/99      5.3G    0.0186   0.01914  0.001081   0.03882       209       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.984       0.851\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     66/99      5.3G   0.01858    0.0191  0.001069   0.03875       220       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.979       0.984       0.851\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     67/99      5.3G   0.01851   0.01892  0.001073    0.0385       230       416: 100% 227/227 [01:43<00:00,  2.20it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.979       0.984       0.851\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     68/99      5.3G   0.01839   0.01906  0.001065   0.03851       246       416: 100% 227/227 [01:43<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.979       0.984       0.852\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     69/99      5.3G   0.01836   0.01885  0.001079   0.03829       246       416: 100% 227/227 [01:43<00:00,  2.19it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.89it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.979       0.984       0.853\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     70/99      5.3G   0.01838   0.01879  0.001093   0.03826       280       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851        0.98       0.984       0.853\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     71/99      5.3G   0.01819   0.01878  0.001045   0.03802       232       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851        0.98       0.985       0.854\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     72/99      5.3G     0.018   0.01863  0.001053   0.03768       284       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852        0.98       0.985       0.855\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     73/99      5.3G   0.01796   0.01841  0.001053   0.03743       263       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852        0.98       0.985       0.855\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     74/99      5.3G   0.01799   0.01833  0.001068   0.03739       220       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.88it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85       0.979       0.984       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     75/99      5.3G   0.01796   0.01842  0.001061   0.03745       217       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.75it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.985       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     76/99      5.3G   0.01791   0.01846  0.001048   0.03742       283       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.86it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.985       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     77/99      5.3G   0.01789   0.01844  0.001057   0.03739       314       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85       0.981       0.985       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     78/99      5.3G   0.01774   0.01834  0.001056   0.03714       244       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851        0.98       0.985       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     79/99      5.3G   0.01764   0.01826  0.001034   0.03693       246       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.84it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851        0.98       0.985       0.856\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     80/99      5.3G   0.01777   0.01828   0.00105   0.03711       250       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03        0.85        0.98       0.985       0.857\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     81/99      5.3G   0.01753   0.01814   0.00103   0.03671       249       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.75it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.981       0.985       0.857\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     82/99      5.3G   0.01764   0.01823  0.001037   0.03691       246       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.78it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.981       0.986       0.857\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     83/99      5.3G   0.01741   0.01782  0.001024   0.03625       233       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.87it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.981       0.986       0.858\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     84/99      5.3G   0.01737   0.01792  0.001031   0.03632       243       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.986       0.858\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     85/99      5.3G   0.01741   0.01793  0.001027   0.03637       239       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.89it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.858\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     86/99      5.3G   0.01728   0.01785  0.001016   0.03615       257       416: 100% 227/227 [01:44<00:00,  2.17it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.81it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.981       0.987       0.858\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     87/99      5.3G   0.01727   0.01774  0.001028   0.03604       232       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.69it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.858\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     88/99      5.3G   0.01714   0.01772  0.001036    0.0359       262       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.92it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.851       0.981       0.987       0.859\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     89/99      5.3G   0.01721   0.01765  0.001047   0.03591       282       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.89it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.859\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     90/99      5.3G   0.01725   0.01774  0.001015   0.03601       239       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.90it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.859\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     91/99      5.3G    0.0171   0.01773  0.001032   0.03586       225       416: 100% 227/227 [01:44<00:00,  2.18it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.85it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.859\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     92/99      5.3G   0.01714   0.01754  0.001039   0.03572       240       416: 100% 227/227 [01:46<00:00,  2.14it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:09<00:00,  2.77it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987        0.86\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     93/99      5.3G   0.01708   0.01746  0.001015   0.03556       220       416: 100% 227/227 [01:44<00:00,  2.16it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987        0.86\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     94/99      5.3G   0.01704   0.01765   0.00101    0.0357       228       416: 100% 227/227 [01:46<00:00,  2.12it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.78it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.853       0.981       0.987        0.86\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     95/99      5.3G   0.01702    0.0176 0.0009974   0.03562       280       416: 100% 227/227 [01:48<00:00,  2.09it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:10<00:00,  2.45it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.853       0.981       0.987        0.86\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     96/99      5.3G   0.01711   0.01761  0.001022   0.03575       246       416: 100% 227/227 [01:47<00:00,  2.12it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.853       0.981       0.987       0.861\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     97/99      5.3G   0.01687   0.01731  0.001015   0.03519       288       416: 100% 227/227 [01:45<00:00,  2.15it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.82it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.852       0.981       0.987       0.861\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     98/99      5.3G    0.0169   0.01736  0.001008   0.03527       234       416: 100% 227/227 [01:45<00:00,  2.15it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:08<00:00,  2.83it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.853       0.981       0.987       0.861\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     99/99      5.3G   0.01679   0.01747 0.0009962   0.03525       238       416: 100% 227/227 [01:45<00:00,  2.15it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:10<00:00,  2.29it/s]\n",
            "                 all    1.56e+03    4.65e+03       0.853       0.981       0.987       0.861\n",
            "Optimizer stripped from runs/train/yolov5s_mundo2/weights/last.pt, 14.7MB\n",
            "Optimizer stripped from runs/train/yolov5s_mundo2/weights/best.pt, 14.7MB\n",
            "100 epochs completed in 3.157 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}