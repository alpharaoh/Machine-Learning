{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0846daa6343784c651a7d9c886b79535d34c1ff46f7f144845b61cf63c85ef614",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "846daa6343784c651a7d9c886b79535d34c1ff46f7f144845b61cf63c85ef614"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## Linear SVM Classification\n",
    "<img src=\"./images/comparison_margin.png\" width=\"1000\">\n",
    "\n",
    "Normal classifiers on left hand side, some are fine for the dataset, but on new data it will be bad (dashed line doesn't even seperate classes, and other lines are too close to the data). On right had side is the Support Vector Machine Classifier. You can see that this classifier allows the most space in-between the data, this is called large margin classification.\n",
    "\n",
    "Note: SVMs are sensitive to the ferature scales, as you can see in this figure below, in the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g. using Scikit-Learns's `StandardScaler`) the decision boundary is the right plot looks much better\n",
    "\n",
    "<img src=\"./images/comparison_feature_scales.png\" width=\"1000\">\n",
    "\n",
    "___\n",
    "\n",
    "## Soft Margin Classification\n",
    "If we want the margin to be on the right hand size rather than the middle we will use hard margin classification. Issues with this approach\n",
    "\n",
    "- only works if data is linearly separable\n",
    "- sensitive to outliers\n",
    "\n",
    "<img src=\"./images/outlier_softmargin.png\" width=\"1000\">\n",
    "\n",
    "The left of this figure shows what the iris dataset would look like with an additional outlier. It is impossible to find a hard margin. On the right, the decision boundary ends up very different from the figure we have seen previously without the outlier. This means that the classifier won't generalize well.\n",
    "\n",
    "To avoid these problems, we can use a more flexible model. We need to find a good balance between keeping the gap as large as possible and limiting the margin violations (i.e., instances that end up in the middle or on the wrong side of the \"road\"). This is called soft margin classification\n",
    "\n",
    "`C` is a hyperparameter when creating a SVM model using Scikit-Learn. If we set it to a low value, then we end up with the model on the left. With a high model, we get the model ont he right. Margin violations are bad. It's usually better to have few of them. however, in this case the model on the left has a lot of margin violations but will probably generalize better.\n",
    "\n",
    "<img src=\"./images/hyperparam.png\" width=\"1000\">\n",
    "\n",
    "Tip: If the SVM model is overfitting, you can try to reduce C\n",
    "\n",
    "___\n",
    "\n",
    "The following code will load the dataset, scale the features and then trains a linear SVM model to detect Iris virginica flowers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "source": [
    "(Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}