{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0a1775983dcf6192600b7921f261b504285134f8d25e4a313c92ba0c2ba37407f",
   "display_name": "Python 3.8.6 64-bit ('tf-gpu': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "a1775983dcf6192600b7921f261b504285134f8d25e4a313c92ba0c2ba37407f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are versitile ML aglorithms that can perform classification, regression and multioutput tasks. Decision trees are also fundamentla components of Random Forests, which are among the most powerful ML aglorithms today.\n",
    "\n",
    "We will discuss how to train, visualize and make predictions. then we will go through CART trainng algorithm used by Scikit-Learn, and we will discuss how to regularise trees and use them for regression tasks. We will also discuss the limitations\n",
    "\n",
    "Lets bult a decision tree using a DecisionTreeClassifier on the iris dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # Petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the export_graphviz method to output a graph definition file called iris_tree.dot\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "# then we can use the dot command line tool from Graphviz package ti convert .dot image to png/pdf etc.\n",
    "# dot -Tpng iris_tree.dot -o iris_tree.png\n"
   ]
  },
  {
   "source": [
    "<img src=\"./iris_tree.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Equation for gini impurity:\n",
    "\n",
    "$G_i=1-\\sum_{k=1}^{n}p_{i,k}^2$\n",
    "\n",
    "Where $p_{i,k}$ is the ratio of class k instances among the training instances in the $i^{th}$ node\n",
    "\n",
    "For example for depth-2 left node has a gini score equal to $1-(0/54)^2-(49/54)^2-(5/54)^2 = 0.168$\n",
    "\n",
    "The Decision Tree can also esitmate the probability that an instance belong sto a particulaar class k. First it traverses the tree to din the lead node for this instance, and then it returns the ratio of training instances of class k in this node. For example, suppose you have found a flower whose petals are 5cm long and 1.5 cm wide. The corresponding leaf node is the depth-2 left node, so decision Tree should output the following probabilites: 0% (0/54) for Iris setosa, 90.7% for Iris versicolor (49/54) and 9.3% for Iris verginica (5/54). Let's check this "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "source": [
    "## CART Training Algorithm\n",
    "\n",
    "Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees. The algorithm works by first splitting the training set into two subsets using a single feature k and a threshold $t_k$ (e.g., \"petal length $\\leq$ 2.5cm\") How does it chooes k and $t_k$? It searches for the pair (k, $t_k$) that produces the purest subsets (weighted by their size).\n",
    "\n",
    "Cost function that CART algorithm tries to minimise:\n",
    "\n",
    "J(k,$t_k$) = $\\dfrac{m_{left}}{m}G_{left} + \\dfrac{m_{right}}{m}G_{right}$\n",
    "\n",
    "Where $G_{left/right}$ measures the impurity of the left/right subset<br>\n",
    "and $m_{left_right}$ measures the number of instances in the left/right subset \n",
    "\n",
    "once it has successfully split the training set into 2, it keeps slitting using the same logic. It stops recusing once it reaches the maximum depth or if it cannot find a split that will reduce impurity. As you can tell, this algorithm is a greedy algorithm - it greedily searches for an optimum split at the top level and repeats the process at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down, often greedy algorithms like this finds a solution that's reasonably good but not optimum.\n",
    "\n",
    "Unfortunately, finding the optimal tree is known to be an NP-Complete problem: it requires O(EXP(m)) time, that's why we have to settle for reasonably good algorithms\n",
    "\n",
    "## Regularization Hyperparameters\n",
    "\n",
    "Decision Trees make very few assumptions about the training data ( as opposed to linear models, which assume that the data is linear ). If left unconstrained, the tree structure will adapt itself to the training data, most likely overfitting it. Such a model is often called a `nonparametric model`, not because it does not have any hyperparameters, but because the number of parameters is not determined prior to training. In contrast, a `parametric model` such as a linear model, has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "To solve this, we can use the `max_depth` hyperparameter (default is None, so means unlimited). It also has other parameters that similarly restict the shape of the Decision Tree: min_samples_split, mon_samples_lead, min_weight_fration_leaf, min_samples_leaf, max_lead_nodes, max_features. Increasing min_* hyperparameters or reducing max_* hyperparameters will regularise the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}